{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5961aa-c5d7-4e9d-9841-9df16bc7ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plumed\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import MDAnalysis as md\n",
    "from MDAnalysis.analysis import distances\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import random\n",
    "import matplotlib\n",
    "from deeptime.decomposition import TICA\n",
    "from deeptime.covariance import KoopmanWeightingEstimator\n",
    "from deeptime.clustering import MiniBatchKMeans\n",
    "from deeptime.markov import TransitionCountEstimator\n",
    "from deeptime.markov.msm import MaximumLikelihoodMSM\n",
    "from deeptime.plots import plot_implied_timescales\n",
    "from deeptime.util.validation import implied_timescales\n",
    "import networkx as nx\n",
    "from copy import deepcopy\n",
    "from numpy.random import multinomial\n",
    "import subprocess\n",
    "import os\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "###############################USER DEFINE REGION##################################\n",
    "\n",
    "### Environment: change this according to your own environment, make sure you can run gmx_mpi/gmx/plumed inside this notebook\n",
    "os.environ['PATH'] = '/usr/local/Climber:/usr/local/pymol:/usr/local/gromacs/bin:/usr/local/plumed/bin:/usr/local/openmpi/bin:/usr/local/cuda-12.2/bin:/usr/local/clash:/home/mingyuan/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/gromacs/lib:/usr/local/plumed/lib:/usr/local/libtorch/lib:/usr/local/openmpi/lib:/usr/local/cuda-12.2/lib64:'\n",
    "\n",
    "### Hyperparameters\n",
    "# Run setup\n",
    "sim_name = 'ala2'\n",
    "colvar = 'CV/COLVAR'\n",
    "topol = 'traj_and_dat/input.pdb'\n",
    "seed_ref = 'traj_and_dat/seed_ref.pdb'\n",
    "n_sim = 16\n",
    "n_steps = 20000\n",
    "# Progress control\n",
    "n_rounds = 20\n",
    "sim_idx = 0\n",
    "# hardware-related\n",
    "ntomp = 1\n",
    "gpu_id = '0,1'\n",
    "n_jobs = 32\n",
    "# TICA parameters (for adaptive sampling)\n",
    "tica_lagtime = 20\n",
    "dim = None\n",
    "var_cutoff = 0.95\n",
    "koopman = True\n",
    "# Markov State Model parameters\n",
    "msm_lagtime = 20\n",
    "# PCCA parameters\n",
    "n_metastable_sets = 30\n",
    "# CV machine learning & convergence check\n",
    "convergence_check = True\n",
    "num_cvs = 2\n",
    "patience = 2\n",
    "convergence_criteria = [0.99,0.95]\n",
    "# OPES parameters\n",
    "barrier = 20\n",
    "opes_steps = 500000\n",
    "\n",
    "def initialize(sim_name,n_sim):\n",
    "    with open('initialize.sh','w+') as f:\n",
    "        f.writelines('gmx grompp -f ../mdp/md.mdp -c ../4-npt/npt.gro -p ../1-topol/topol.top -o {sim_name}.tpr\\n'.format(sim_name=sim_name))\n",
    "        f.writelines('for i in `seq 0 {n_sim_0}`\\n'.format(n_sim_0=n_sim-1))\n",
    "        f.writelines('do\\n')\n",
    "        f.writelines('    mkdir $i\\n')\n",
    "        f.writelines('    cp {sim_name}.tpr $i\\n'.format(sim_name=sim_name))\n",
    "        f.writelines('done\\n')\n",
    "        f.writelines('mkdir CV traj_and_dat figures opes\\n')\n",
    "        f.writelines('mv {sim_name}.tpr opes/\\n'.format(sim_name=sim_name))\n",
    "    subprocess.run(['chmod u+x initialize.sh'], shell=True)\n",
    "    subprocess.run(['./initialize.sh'], shell=True)\n",
    "    return None\n",
    "\n",
    "def gmx_mpirun(sim_name,sim_idx,n_sim,n_steps,ntomp,gpu_id):\n",
    "    multidir = ''\n",
    "    idx_start = sim_idx * n_sim\n",
    "    idx_end = (sim_idx + 1) * n_sim - 1\n",
    "    for i in range(idx_start,idx_end+1):\n",
    "        multidir = multidir + str(i) + ' '\n",
    "    multidir = multidir[:-1]\n",
    "    with open('mpirun.sh','w+') as f:\n",
    "        f.writelines('mpirun -np {n_sim} -cpus-per-rank {ntomp} gmx_mpi mdrun -v -deffnm {sim_name} -multidir {multidir} -pme gpu -nb gpu -bonded gpu -nsteps {n_steps} -ntomp {ntomp} -gpu_id {gpu_id} -update gpu'.format(n_sim=n_sim,ntomp=ntomp,sim_name=sim_name,multidir=multidir,n_steps=n_steps,gpu_id=gpu_id))\n",
    "    subprocess.run(['chmod u+x mpirun.sh'],shell=True)\n",
    "    subprocess.run(['./mpirun.sh'],shell=True)\n",
    "    return None\n",
    "    \n",
    "# Feature functions\n",
    "def select_dihedrals(universe,dihedral_type,start_res,end_res):\n",
    "    dihedrals={}\n",
    "    if 'phi' in dihedral_type:\n",
    "        dihedrals['phi']=[]\n",
    "        for i in range(start_res,end_res+1):\n",
    "            if i != universe.residues.resids[0]:         # last residue does not have phi (if not capped)\n",
    "                dihedrals['phi'].append(i)\n",
    "\n",
    "    if 'psi' in dihedral_type:\n",
    "        dihedrals['psi']=[]\n",
    "        for i in range(start_res,end_res+1):\n",
    "            if i != universe.residues.resids[-1]:          # first residue does not have psi (if not capped)\n",
    "                dihedrals['psi'].append(i)\n",
    "\n",
    "    if 'omega' in dihedral_type:\n",
    "        dihedrals['omega']=[]\n",
    "        for i in range(start_res,end_res+1):\n",
    "            if i != universe.residues.resids[0]:          # first residue does not have omega (if not capped)\n",
    "                dihedrals['omega'].append(i)\n",
    "\n",
    "    if 'chi1' in dihedral_type:\n",
    "        dihedrals['chi1']=[]\n",
    "        for i in range(start_res,end_res+1):\n",
    "            count = i - universe.residues.resids[0]\n",
    "            if universe.residues.resnames[count] not in ['GLY','ALA']:\n",
    "                dihedrals['chi1'].append(i)\n",
    "\n",
    "    if 'chi2' in dihedral_type:\n",
    "        dihedrals['chi2']=[]\n",
    "        for i in range(start_res,end_res+1):\n",
    "            count = i - universe.residues.resids[0]\n",
    "            if universe.residues.resnames[count] not in ['GLY','ALA','CYS','SER','THR','VAL']:\n",
    "                dihedrals['chi2'].append(i)\n",
    "\n",
    "    if 'chi3' in dihedral_type:\n",
    "        dihedrals['chi3']=[]\n",
    "        for i in range(start_res,end_res+1):\n",
    "            count = i - universe.residues.resids[0]\n",
    "            if universe.residues.resnames[count] in ['ARG','GLN','GLU','LYS','MET']:\n",
    "                dihedrals['chi3'].append(i)\n",
    "\n",
    "    if 'chi4' in dihedral_type:\n",
    "        dihedrals['chi4']=[]\n",
    "        for i in range(start_res,end_res+1):\n",
    "            count = i - universe.residues.resids[0]\n",
    "            if universe.residues.resnames[count] in ['ARG','LYS']:\n",
    "                dihedrals['chi4'].append(i)\n",
    "    return dihedrals\n",
    "    \n",
    "def write_features(colvar,sim_idx,n_sim,sim_name):\n",
    "    idx_start = sim_idx * n_sim\n",
    "    idx_end = (sim_idx+1) * n_sim - 1\n",
    "    with open('features.sh','w+') as f:\n",
    "        f.writelines('start={idx_start}\\n'.format(idx_start=idx_start))\n",
    "        f.writelines('end={idx_end}\\n'.format(idx_end=idx_end))\n",
    "        f.writelines('for i in `seq $start $end`\\n')\n",
    "        f.writelines('do\\n')\n",
    "        f.writelines('    cp $i/{sim_name}.xtc traj_and_dat/\\n'.format(sim_name=sim_name))\n",
    "        f.writelines('    mv traj_and_dat/{sim_name}.xtc traj_and_dat/$i.xtc\\n'.format(sim_name=sim_name))\n",
    "        f.writelines('    plumed driver --plumed traj_and_dat/features.dat --mf_xtc traj_and_dat/$i.xtc\\n')\n",
    "        f.writelines('    plumed driver --plumed traj_and_dat/ref.dat --mf_xtc traj_and_dat/$i.xtc\\n')\n",
    "        f.writelines('    mv COLVAR CV/COLVAR_$i\\n')\n",
    "        f.writelines('    mv COLVAR_ref CV/COLVAR_ref_$i\\n')\n",
    "        f.writelines('done\\n')\n",
    "    subprocess.run(['chmod u+x features.sh'], shell=True)\n",
    "    subprocess.run(['./features.sh'], shell=True)\n",
    "    return None\n",
    "\n",
    "### Analysis functions\n",
    "def read_features(colvar,sim_idx,n_sim):\n",
    "    # traj is the time-series COLVAR in pandas.DataFrame format\n",
    "    traj_idx = []\n",
    "    for i in range(sim_idx*n_sim):\n",
    "        traj_idx.append(i)\n",
    "    \n",
    "    no_traj = len(traj_idx)\n",
    "    traj = [0]*no_traj\n",
    "    \n",
    "    for i in traj_idx:\n",
    "        traj[i] = plumed.read_as_pandas(colvar+'_{i}'.format(i=i))\n",
    "        traj[i] = traj[i].drop(columns=['time'])\n",
    "        columns = list(traj[i].columns.values)\n",
    "        # Remove all dihedral angles, only keep sin/cos dihedrals \n",
    "        for column in columns:\n",
    "            if column[:3] == 'phi' or column[:3] == 'psi' or column[:3] == 'chi' or column[:5] == 'omega':\n",
    "                traj[i] = traj[i].drop(columns=[column])\n",
    "            \n",
    "    # data is the time-series COLVAR in numpy.ndarrays format\n",
    "    data = [0]*len(traj)\n",
    "    for i in range(len(traj)):\n",
    "        numpy_data = traj[i].to_numpy(dtype='float32')\n",
    "        data[i] = numpy_data\n",
    "            \n",
    "    return traj,data\n",
    "\n",
    "def data_supplement(sim_idx,data,lagtime):\n",
    "    round_seed_idx = []\n",
    "\n",
    "    for i in range(1,sim_idx):\n",
    "        round_seed_idx_i = np.loadtxt('round{i}_seed.txt'.format(i=i),dtype=int)\n",
    "        round_seed_idx.append(round_seed_idx_i)\n",
    "\n",
    "    round_seed_idx = np.concatenate(round_seed_idx)\n",
    "\n",
    "    data_supp = []\n",
    "\n",
    "    for i,round_seed_idx_i in enumerate(round_seed_idx):\n",
    "    \n",
    "        sim_i = round_seed_idx_i[0]\n",
    "        frame = round_seed_idx_i[1]\n",
    "    \n",
    "        if frame == 0:\n",
    "            continue\n",
    "        elif lagtime > frame:\n",
    "            start_frame = 0\n",
    "        else:\n",
    "            start_frame = frame - lagtime + 1\n",
    "    \n",
    "        end_frame = lagtime\n",
    "    \n",
    "        data_supp_pre = data[sim_i][start_frame:frame,:]\n",
    "        data_supp_post = data[i][:end_frame,:]\n",
    "        data_supp_i = np.concatenate([data_supp_pre,data_supp_post])\n",
    "    \n",
    "        data_supp.append(data_supp_i)\n",
    "    \n",
    "    return data_supp\n",
    "    \n",
    "def run_TICA(data,data_supp,lagtime,dim=None,var_cutoff=None,koopman=True):\n",
    "    data_syn = data + data_supp\n",
    "    tica = TICA(lagtime=lagtime,dim=dim,var_cutoff=var_cutoff)\n",
    "    if koopman == True:\n",
    "        koopman_estimator = KoopmanWeightingEstimator(lagtime=lagtime)\n",
    "        reweighting_model = koopman_estimator.fit(data_syn).fetch_model()\n",
    "        tica = tica.fit(data_syn, weights=reweighting_model).fetch_model()\n",
    "    else:\n",
    "        tica = tica.fit(data_syn).fetch_model()\n",
    "    # tica is the data-fitted model, which contains eigenvalues and eigenvectors\n",
    "    # tica_output is the tranformed time-series data in TICA space in shape(traj_idx,no_frames,dim)\n",
    "    # tica_output_concat is tica_output in shape(traj_idx*no_frames,dim)\n",
    "    tica_output = tica.transform(data)\n",
    "    tica_output_concat = np.concatenate(tica_output)\n",
    "\n",
    "    tica_output_supp = []\n",
    "    for data_supp_i in data_supp:\n",
    "        tica_output_supp_i = tica.transform(data_supp_i)\n",
    "        tica_output_supp.append(tica_output_supp_i)\n",
    "        \n",
    "    return tica,tica_output,tica_output_concat,tica_output_supp\n",
    "\n",
    "def calculate_nmicro(data_concat):\n",
    "    # Heuristic approach to determine cluster number from htmd \n",
    "    # https://github.com/Acellera/htmd/blob/master/htmd/adaptive/adaptivebandit.py\n",
    "    n_microstates = int(max(100, np.round(0.6 * np.log10(data_concat.shape[0] / 1000) * 1000 + 50)))\n",
    "    return n_microstates\n",
    "    \n",
    "def run_kmeans(tica_output,tica_output_supp,tica_output_concat,n_microstates,n_jobs):\n",
    "    minibatch_kmeans = MiniBatchKMeans(n_clusters=n_microstates,batch_size=10000,max_iter=100,init_strategy='kmeans++',n_jobs=n_jobs)\n",
    "    microstates = minibatch_kmeans.fit(tica_output_concat).fetch_model()\n",
    "    cluster_centers = microstates.cluster_centers\n",
    "    # assignments_concat is the microstate assignment in shape (traj_idx*no_frames,)\n",
    "    # assignments is the microstate assignment in shape (traj_idx,no_frames)\n",
    "    assignments_concat = microstates.transform(tica_output_concat)\n",
    "    assignments = assignments_concat.reshape(-1,tica_output.shape[1])\n",
    "\n",
    "    assignments_supp = []\n",
    "    for tica_output_supp_i in tica_output_supp:\n",
    "        assignments_supp_i = microstates.transform(tica_output_supp_i)\n",
    "        assignments_supp.append(assignments_supp_i)\n",
    "    \n",
    "    return assignments,assignments_concat,cluster_centers,assignments_supp\n",
    "    \n",
    "def build_MSM(msm_lagtime,assignments,assignments_supp):\n",
    "    assignments_syn = list(assignments) + assignments_supp\n",
    "    counts = TransitionCountEstimator(lagtime=msm_lagtime, count_mode='sliding').fit_fetch(assignments_syn)\n",
    "    msm = MaximumLikelihoodMSM().fit_fetch(counts)\n",
    "    return counts,msm\n",
    "    \n",
    "def run_PCCA(msm,n_metastable_sets):\n",
    "    pcca = msm.pcca(n_metastable_sets=n_metastable_sets)\n",
    "    return pcca\n",
    "\n",
    "### Adaptive seeding functions\n",
    "def fix_disconnected(counts,n_microstates,msm,pcca):\n",
    "    ### Locate the connected and disconnected sets:\n",
    "    sets = counts.connected_sets(connectivity_threshold=0,directed=True,sort_by_population=True)\n",
    "    disconnected_sets = sets[1:]\n",
    "    n_macro_disconnected = len(disconnected_sets)\n",
    "    disconnected_dict = {}\n",
    "    for i in range(n_macro_disconnected):\n",
    "        macro_label = n_metastable_sets + i\n",
    "        for j in disconnected_sets[i]:\n",
    "            disconnected_dict[j] = macro_label\n",
    "\n",
    "    # We need to modify:\n",
    "    # 1. pcca.assignments: assign each disconnected microstate to a new pseudo-macrostate\n",
    "    # 2. msm.stationary_distribution: insert 0 at the location corresponding to the disconnected microstates\n",
    "    pcca_assignments = np.zeros(n_microstates,dtype=int)\n",
    "    stationary_distribution = np.zeros(n_microstates,)\n",
    "\n",
    "    connected_count = 0\n",
    "    for i in range(n_microstates):\n",
    "        if i in disconnected_dict.keys():\n",
    "            pcca_assignments[i] = disconnected_dict[i]\n",
    "            stationary_distribution[i] = 0\n",
    "        else:\n",
    "            pcca_assignments[i] = pcca.assignments[connected_count]\n",
    "            stationary_distribution[i] = msm.stationary_distribution[connected_count]\n",
    "            connected_count += 1\n",
    "        \n",
    "    return n_macro_disconnected,pcca_assignments,stationary_distribution\n",
    "    \n",
    "    \n",
    "def count_macro(n_sim,n_macro_disconnected,pcca_assignments,assignments,assignments_concat,sim_idx):\n",
    "    # Obtain macrostate assignment to original time-series data\n",
    "    macro_assignments = dict(enumerate(pcca_assignments))\n",
    "    macro_timeseries = np.vectorize(macro_assignments.get)(assignments_concat)\n",
    "    \n",
    "    # Macrostate seeding\n",
    "    unique_macro, counts_macro = np.unique(macro_timeseries, return_counts=True)\n",
    "    prob_macro = (1 / counts_macro) / np.sum(1 / counts_macro)\n",
    "    macrostate_seed = multinomial(n_sim,prob_macro)\n",
    "\n",
    "    # Microstate seeding\n",
    "    # First count the occurences of all microstates\n",
    "    unique_micro, counts_micro = np.unique(assignments_concat, return_counts=True)\n",
    "    seed_idx = []\n",
    "    counts_micro_i_log = {}\n",
    "    for macro_i, n_sample in enumerate(macrostate_seed):\n",
    "        # locate the index of microstates not assigned to current selected macrostates\n",
    "        not_macro_idx = np.where(pcca_assignments != np.unique(pcca_assignments)[macro_i])\n",
    "        # let all entries corresponding to not_macro_idx = 0, therefore ignore them during selection\n",
    "        counts_micro_i = deepcopy(counts_micro)\n",
    "        counts_micro_i[not_macro_idx] = 0\n",
    "        # let 1/0 = 0\n",
    "        inverse_counts = np.where(counts_micro_i==0, 0, 1/counts_micro_i)\n",
    "        prob_micro_i = inverse_counts / np.sum(inverse_counts)\n",
    "        microstate_seed = multinomial(n_sample,prob_micro_i)\n",
    "        # Record selection statistics for visualization\n",
    "        if n_sample != 0:\n",
    "            macro_idx_log = unique_macro[macro_i] \n",
    "            counts_micro_i_log[macro_idx_log] = [counts_micro_i,microstate_seed]\n",
    "        for micro_i, n_sample in enumerate(microstate_seed):\n",
    "            seed_idx = seed_idx + n_sample * [micro_i]\n",
    "\n",
    "    conf_seed = []\n",
    "\n",
    "    for i,seed in enumerate(seed_idx):\n",
    "        conf_idx = np.array(np.where(assignments==seed)).T\n",
    "        conf_seed_frame = conf_idx[np.random.randint(conf_idx.shape[0], size=1), :][0]\n",
    "        conf_seed.append(conf_seed_frame)\n",
    "\n",
    "    # Visualization\n",
    "    n_macro_nodes = n_metastable_sets + n_macro_disconnected\n",
    "    labels = {}\n",
    "    color_map = []\n",
    "    node_size = []\n",
    "\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # label and color for origin\n",
    "    labels[0] = assignments_concat.shape[0]\n",
    "    color_map.append('red')\n",
    "\n",
    "    # label,color,node size\n",
    "    for i,macro_idx in enumerate(unique_macro):\n",
    "        seed_counts_i = macrostate_seed[i]\n",
    "        labels[macro_idx+1] = str(counts_macro[i])+',{seed_counts_i}'.format(seed_counts_i=seed_counts_i)\n",
    "        if seed_counts_i != 0:\n",
    "            color_map.append('orange')\n",
    "        else:\n",
    "            color_map.append('blue')\n",
    "        G.add_edge(0,macro_idx+1)\n",
    "\n",
    "    for macro_i,micro_counts_seed in counts_micro_i_log.items():\n",
    "        index = np.where(micro_counts_seed[0] != 0)\n",
    "        for idx in index[0]:\n",
    "            if idx <= n_macro_nodes:\n",
    "                G.add_edge(macro_i+1,idx+10000)\n",
    "                labels[idx+10000] = str(micro_counts_seed[0][idx])+','+str(micro_counts_seed[1][idx])\n",
    "            else:\n",
    "                G.add_edge(macro_i+1,idx)\n",
    "                labels[idx] = str(micro_counts_seed[0][idx])+','+str(micro_counts_seed[1][idx])\n",
    "            if micro_counts_seed[1][idx] != 0:\n",
    "                color_map.append('green')\n",
    "            else:\n",
    "                color_map.append('blue')\n",
    "\n",
    "    for i,node in enumerate(G):\n",
    "        node_size.append(1000)\n",
    "\n",
    "    # get positions\n",
    "    pos = nx.spring_layout(G)\n",
    "\n",
    "    # shift position a little bit\n",
    "    shift = [0.1, 0]\n",
    "    shifted_pos ={node: node_pos + shift for node, node_pos in pos.items()}\n",
    "\n",
    "    # adjust size\n",
    "    fig,ax = plt.subplots(figsize=(14,14))\n",
    "    #ax.set_xlim([1*x for x in axis.get_xlim()])\n",
    "    #ax.set_ylim([1*y for y in axis.get_ylim()])\n",
    "\n",
    "    # draw graph\n",
    "    nx.draw(G, pos, with_labels=True,font_color='white',node_color=color_map,node_size=node_size)\n",
    "\n",
    "    # draw labels\n",
    "    nx.draw_networkx_labels(G, shifted_pos, labels=labels, horizontalalignment=\"left\")\n",
    "\n",
    "    # turn off frame\n",
    "    ax.axis(\"off\")\n",
    "    # Save figure\n",
    "    plt.savefig('figures/Visualize_Count_Macro_{sim_idx}.png'.format(sim_idx=sim_idx),dpi=600)\n",
    "        \n",
    "    return conf_seed\n",
    "    \n",
    "    \n",
    "def write_gmxfile(sim_idx,n_sim,seed_ref,conf_seed):\n",
    "    ### .gro seed files generation\n",
    "    u_list = []\n",
    "    for i in range(sim_idx*n_sim):\n",
    "        u_traj = md.Universe(seed_ref,'traj_and_dat/{i}.xtc'.format(i=i))\n",
    "        u_list.append(u_traj)\n",
    "\n",
    "    for i,seed in enumerate(conf_seed):\n",
    "        traj_no = seed[0]\n",
    "        frame = seed[1]\n",
    "        u_list[traj_no].atoms.write('{i}.gro'.format(i=i+sim_idx*n_sim),frames=u_list[traj_no].trajectory[frame:frame+1])\n",
    "\n",
    "    np.savetxt('round{sim_idx}_seed.txt'.format(sim_idx=sim_idx),conf_seed,fmt='%s')\n",
    "        \n",
    "    idx_start = sim_idx * n_sim\n",
    "    idx_end = (sim_idx + 1) * n_sim - 1\n",
    "        \n",
    "    with open('grompp.sh','w+') as f:\n",
    "        f.writelines('start={idx_start}\\n'.format(idx_start=idx_start))\n",
    "        f.writelines('end={idx_end}\\n'.format(idx_end=idx_end))\n",
    "        f.writelines('for i in `seq $start $end`\\n')\n",
    "        f.writelines('do\\n')\n",
    "        f.writelines('    mkdir $i\\n')\n",
    "        f.writelines('    gmx grompp -f ../mdp/md.mdp -p ../1-topol/topol.top -c $i.gro -o {sim_name}.tpr\\n'.format(sim_name=sim_name))\n",
    "        f.writelines('    mv {sim_name}.tpr $i\\n'.format(sim_name=sim_name))\n",
    "        f.writelines('done\\n')\n",
    "    subprocess.run(['chmod u+x grompp.sh'], shell=True)\n",
    "    subprocess.run(['./grompp.sh'], shell=True)\n",
    "        \n",
    "    return None\n",
    "\n",
    "### Machine learning CV related functions\n",
    "def cv_ref_projection(): # TODO: print figures which project tica eigenvectors on 2d ref surface \n",
    "    return\n",
    "\n",
    "def calculate_cv_sigma(tica_output_concat,num_cvs):\n",
    "    sigma = np.zeros(num_cvs,)\n",
    "    for i in range(num_cvs):\n",
    "        sigma[i] = np.std(tica_output_concat.T[i])\n",
    "    return sigma\n",
    "    \n",
    "def cv_convergence(data,tica_lagtime,num_cvs,tica_cv_models,sim_idx):\n",
    "    # We use all TICA models learnt to transform current dataset\n",
    "    tica_outputs = []\n",
    "\n",
    "    for tica_i in tica_cv_models:\n",
    "        tica_output_concat_i = np.concatenate(tica_i.transform(data))\n",
    "        tica_outputs.append(tica_output_concat_i)\n",
    "\n",
    "    # Stack all transformation into a 3d array with shape (model_no,frame_no,num_cvs)\n",
    "    tica_outputs = np.stack(tica_outputs)\n",
    "\n",
    "    # Check convergence with Pearson correlation\n",
    "    correlations = np.zeros((tica_outputs.shape[0]-1,num_cvs))\n",
    "    \n",
    "    for i in range(tica_outputs.shape[0]-1):\n",
    "        for j in range(num_cvs):\n",
    "            correlation = pearsonr(tica_outputs[i,:,j].T,tica_outputs[-1,:,j].T)[0]\n",
    "            correlations[i][j] = np.abs(correlation)\n",
    "\n",
    "    return correlations\n",
    "\n",
    "def tica_plumed(barrier,feature_dat,traj,tica,num_cvs,sigma):\n",
    "    traj_concat = pd.concat(traj,axis=0)\n",
    "    with open('opes/plumed.dat','w+') as f:\n",
    "        f.writelines('MOLINFO STRUCTURE=input.pdb\\n')\n",
    "        with open(feature_dat,'r') as g:\n",
    "            lines = g.readlines()\n",
    "            for line in lines[1:-1]:   # remove PRINT argument\n",
    "                f.writelines(line)\n",
    "                \n",
    "        arg_string = ''\n",
    "        for feature in traj_concat.columns:\n",
    "            arg_string = arg_string + feature + ','\n",
    "        arg_string = arg_string[:-1]\n",
    "\n",
    "        parameters_string = np.array2string(tica.mean_0,separator=',',threshold=np.inf,max_line_width=np.inf,floatmode='fixed')[1:-1]\n",
    "        \n",
    "        for i in range(num_cvs):\n",
    "            coeff_string = ''\n",
    "            for value in tica.singular_vectors_left.T[i]:\n",
    "                string = str(value)+','\n",
    "                coeff_string = coeff_string + string\n",
    "            coeff_string = coeff_string[:-1]\n",
    "            f.writelines('tica{i}: COMBINE ARG={arg_string} COEFFICIENTS={coeff_string} PARAMETERS={parameters_string} PERIODIC=NO\\n'.format(i=i,arg_string=arg_string,coeff_string=coeff_string,parameters_string=parameters_string))\n",
    "        for feature in traj_concat.columns:\n",
    "            arg_string = arg_string + feature + ','\n",
    "        arg_string = arg_string[:-1]\n",
    "\n",
    "        ####\n",
    "        f.writelines('phi: TORSION ATOMS=@phi-2\\n')\n",
    "        f.writelines('psi: TORSION ATOMS=@psi-2\\n')\n",
    "        ####\n",
    "        \n",
    "        f.writelines('opes: OPES_METAD ...\\n')\n",
    "        opes_arg = ''\n",
    "        for i in range(num_cvs):\n",
    "            opes_arg = opes_arg + 'tica{i}'.format(i=i) + ','\n",
    "        opes_arg = opes_arg[:-1]\n",
    "        f.writelines('    ARG={opes_arg}\\n'.format(opes_arg=opes_arg))\n",
    "        f.writelines('    PACE=500 BARRIER={barrier}\\n'.format(barrier=barrier))\n",
    "        sigma_string = ''\n",
    "        for i in range(num_cvs):\n",
    "            sigma_string = sigma_string + str(sigma[i]) + ','\n",
    "        sigma_string = sigma_string[:-1]\n",
    "        f.writelines('    SIGMA={sigma_string}\\n'.format(sigma_string=sigma_string))\n",
    "        f.writelines('    STATE_RFILE=../STATE\\n')\n",
    "        f.writelines('    STATE_WFILE=../STATE\\n')\n",
    "        f.writelines('    NLIST\\n')\n",
    "        f.writelines('    WALKERS_MPI\\n')\n",
    "        f.writelines('...\\n')\n",
    "        f.writelines('PRINT ARG=phi,psi,{opes_arg},opes.* STRIDE=500 FILE=COLVAR'.format(opes_arg=opes_arg))\n",
    "        \n",
    "    subprocess.run(['cp traj_and_dat/input.pdb opes/'], shell=True)\n",
    "\n",
    "    return None\n",
    "\n",
    "# OPES related\n",
    "def opes_seed(tica_output,tica_output_concat,tica_output_supp,n_sim,n_jobs,sim_idx,seed_ref):\n",
    "    # 2D clustering\n",
    "    assignments,assignments_concat,cluster_centers,assignments_supp = run_kmeans(tica_output,tica_output_supp,tica_output_concat,n_sim,n_jobs)\n",
    "    \n",
    "    # Visualization\n",
    "    fig,ax = plt.subplots()\n",
    "    cmap = matplotlib.colors.ListedColormap(['black','indigo','darkslateblue','steelblue','teal','darkcyan','lightseagreen','mediumseagreen','slategrey','yellowgreen','greenyellow','gold','yellow','darkviolet','violet','pink'])\n",
    "    norm = matplotlib.colors.BoundaryNorm(np.arange(-0.5,16), cmap.N) \n",
    "    sc = ax.scatter(tica_output_concat[:,0],tica_output_concat[:,1],c=assignments_concat,cmap=cmap,norm=norm,s=8)\n",
    "    plt.plot(cluster_centers[:,0],cluster_centers[:,1],'o',ms=5,color='red')\n",
    "    plt.colorbar(sc,label='cluster',ticks=np.linspace(0,15,16))\n",
    "    plt.xlabel(r'KTICA tIC1 $\\hat \\phi_1$')\n",
    "    plt.ylabel(r'KTICA tIC2 $\\hat \\phi_2$')\n",
    "    plt.savefig('figures/OPES_seed.png',dpi=600,bbox_inches='tight')\n",
    "    \n",
    "    # Seeding\n",
    "    \n",
    "    opes_seed_idx = []\n",
    "    \n",
    "    for i in range(n_sim):\n",
    "        dist_to_center_i = np.linalg.norm(tica_output - cluster_centers[i],axis=2)\n",
    "        opes_seed_idx_i = np.array([np.where(dist_to_center_i == dist_to_center_i.min())[0][0],np.where(dist_to_center_i == dist_to_center_i.min())[1][0]])\n",
    "        opes_seed_idx.append(opes_seed_idx_i)\n",
    "\n",
    "    u_list = []\n",
    "    for i in range(sim_idx*n_sim):\n",
    "        u_traj = md.Universe(seed_ref,'traj_and_dat/{i}.xtc'.format(i=i))\n",
    "        u_list.append(u_traj)\n",
    "\n",
    "    for i,seed in enumerate(opes_seed_idx):\n",
    "        traj_no = seed[0]\n",
    "        frame = seed[1]\n",
    "        u_list[traj_no].atoms.write('opes/{i}.gro'.format(i=i),frames=u_list[traj_no].trajectory[frame:frame+1])\n",
    "\n",
    "    np.savetxt('opes/opes_seed.txt',opes_seed_idx,fmt='%s')\n",
    "    \n",
    "    return None\n",
    "\n",
    "def opes_mpirun(opes_steps,sim_idx,n_steps,n_sim,ntomp,sim_name,gpu_id,num_cvs,barrier):\n",
    "    with open('opes/opes_grompp.sh','w+') as f:\n",
    "        f.writelines('for i in `seq 0 {n_sim}`\\n'.format(n_sim=n_sim-1))\n",
    "        f.writelines('do\\n')\n",
    "        f.writelines('    mkdir opes/$i\\n')\n",
    "        f.writelines('    gmx grompp -f ../mdp/md.mdp -p ../1-topol/topol.top -c opes/$i.gro -o opes/{sim_name}.tpr\\n'.format(sim_name=sim_name))\n",
    "        f.writelines('    mv opes/{sim_name}.tpr opes/$i.gro opes/$i\\n'.format(sim_name=sim_name))\n",
    "        f.writelines('    cp traj_and_dat/input.pdb opes/$i/\\n')\n",
    "        f.writelines('    cp opes/plumed.dat opes/$i/\\n')\n",
    "        f.writelines('done\\n')\n",
    "    subprocess.run(['chmod u+x opes/opes_grompp.sh'], shell=True)\n",
    "    subprocess.run(['./opes/opes_grompp.sh'], shell=True)\n",
    "    \n",
    "    if opes_steps == None:\n",
    "        opes_steps = sim_idx * n_steps\n",
    "    \n",
    "    multidir = ''\n",
    "    \n",
    "    for i in range(0,n_sim):\n",
    "        multidir = multidir + str(i) + ' '\n",
    "    multidir = multidir[:-1]\n",
    "    \n",
    "    with open('opes/mpirun_opes.sh','w+') as f:\n",
    "        f.writelines('cd opes/\\n')\n",
    "        f.writelines('mpirun -np {n_sim} -cpus-per-rank {ntomp} gmx_mpi mdrun -v -deffnm {sim_name} -multidir {multidir} -pme gpu -nb gpu -bonded gpu -nsteps {opes_steps} -ntomp {ntomp} -gpu_id {gpu_id} -plumed plumed.dat -cpi\\n'.format(n_sim=n_sim,ntomp=ntomp,sim_name=sim_name,multidir=multidir,opes_steps=opes_steps,gpu_id=gpu_id))\n",
    "\n",
    "    subprocess.run(['chmod u+x ./opes/mpirun_opes.sh'],shell=True)\n",
    "    subprocess.run(['./opes/mpirun_opes.sh'],shell=True)\n",
    "\n",
    "    return None\n",
    "\n",
    "### Main program\n",
    "\n",
    "# Initialization\n",
    "tica_cv_models = []\n",
    "data_supp = []\n",
    "\n",
    "# Retrieve all the past tica models if convergence check is required\n",
    "if sim_idx != 0 and convergence_check == True:\n",
    "    \n",
    "    traj,data = read_features(colvar,sim_idx,n_sim)   # To do: This is repetitive\n",
    "    if sim_idx != 1:\n",
    "        data_supp = data_supplement(sim_idx,data,tica_lagtime)\n",
    "    \n",
    "    for sim_i in range(1,sim_idx+1):\n",
    "        data_sim_i = data[:(sim_i*n_sim)]\n",
    "        if sim_i == 1:\n",
    "            data_supp_i = []\n",
    "        else:\n",
    "            data_supp_i = data_supplement(sim_i,data_sim_i,tica_lagtime)\n",
    "        data_syn_i = data_sim_i + data_supp_i\n",
    "        tica_cv_model_i = run_TICA(data_sim_i,data_syn_i,tica_lagtime,num_cvs,None,koopman)[0]\n",
    "        tica_cv_models.append(tica_cv_model_i)\n",
    "\n",
    "# First round parallel simulation\n",
    "if sim_idx == 0:\n",
    "    \n",
    "    # Initialize files and directories\n",
    "    initialize(sim_name,n_sim)\n",
    "    gmx_mpirun(sim_name,sim_idx,n_sim,n_steps,ntomp,gpu_id)\n",
    "\n",
    "    ### Write topol and seed_ref file\n",
    "    subprocess.run(['echo 1 1 | gmx trjconv -f 0/{sim_name}.xtc -s 0/{sim_name}.tpr -o {topol} -pbc mol -ur compact -center -dump 0'.format(sim_name=sim_name,topol=topol)],shell=True)\n",
    "    subprocess.run(['echo 0 0 | gmx trjconv -f 0/{sim_name}.xtc -s 0/{sim_name}.tpr -o {seed_ref} -pbc mol -ur compact -center -dump 0'.format(sim_name=sim_name,seed_ref=seed_ref)],shell=True)\n",
    "\n",
    "    #########################################USER DEFINE REGION######################################################\n",
    "\n",
    "    u = md.Universe('traj_and_dat/input.pdb')\n",
    "    heavy_atom = u.select_atoms('name CA or name CB or name O or name N or name CH3 or name C')\n",
    "    ids = list(heavy_atom.ids)\n",
    "    atom_pairs = list(itertools.combinations(ids, 2))\n",
    "\n",
    "    # Features\n",
    "    with open('traj_and_dat/features.dat','w+') as f:\n",
    "        f.writelines('MOLINFO STRUCTURE=traj_and_dat/input.pdb\\n')\n",
    "        count = 0\n",
    "        for pair in atom_pairs:\n",
    "            atom1 = pair[0]\n",
    "            atom2 = pair[1]\n",
    "            count = count + 1\n",
    "            f.writelines('pair{count}: DISTANCE ATOMS={atom1},{atom2}\\n'.format(count=count,atom1=atom1,atom2=atom2))\n",
    "        f.writelines('PRINT ARG=* STRIDE=1 FILE=COLVAR')\n",
    "\n",
    "    # Physical intuitive CV as reference\n",
    "    with open('traj_and_dat/ref.dat','w+') as f:\n",
    "        f.writelines('MOLINFO STRUCTURE=traj_and_dat/input.pdb\\n')\n",
    "        f.writelines('phi: TORSION ATOMS=@phi-2\\n')\n",
    "        f.writelines('psi: TORSION ATOMS=@psi-2\\n')\n",
    "        f.writelines('PRINT ARG=* STRIDE=1 FILE=COLVAR_ref\\n')\n",
    "    \n",
    "    #################################################################################################################\n",
    "\n",
    "    # Write features\n",
    "    write_features(colvar,sim_idx,n_sim,sim_name)\n",
    "\n",
    "    sim_idx = sim_idx + 1\n",
    "\n",
    "    ### CV convergence check\n",
    "    if convergence_check == True:\n",
    "        traj,data = read_features(colvar,sim_idx,n_sim)        # TODO: This is a repetitive action\n",
    "        tica_0 = run_TICA(data,data_supp,tica_lagtime,num_cvs,None,koopman)[0]\n",
    "        tica_cv_models.append(tica_0)\n",
    "\n",
    "# Adaptive Sampling starts from round 1\n",
    "for sim_idx in range(sim_idx,n_rounds):\n",
    "    ### Adaptive sampling\n",
    "    # Read features\n",
    "    traj,data = read_features(colvar,sim_idx,n_sim)\n",
    "    if sim_idx > 1:\n",
    "        data_supp = data_supplement(sim_idx,data,tica_lagtime)\n",
    "    # Perform TICA\n",
    "    tica,tica_output,tica_output_concat,tica_output_supp = run_TICA(data,data_supp,tica_lagtime,dim,var_cutoff,koopman)\n",
    "    # K-means clustering\n",
    "    n_microstates = calculate_nmicro(tica_output_concat)\n",
    "    assignments,assignments_concat,cluster_centers,assignments_supp = run_kmeans(tica_output,tica_output_supp,tica_output_concat,n_microstates,n_jobs)\n",
    "    counts,msm = build_MSM(msm_lagtime,assignments,assignments_supp)\n",
    "    # Lumping microstates into macrostates by PCCA. NB: THIS STEP IS VERY SLOW. BE PATIENT.\n",
    "    n_metastable_sets_tmp = min(n_metastable_sets, msm.transition_matrix.shape[0])\n",
    "    pcca = run_PCCA(msm,n_metastable_sets_tmp)\n",
    "    # Seed generation\n",
    "    n_macro_disconnected,pcca_assignments,stationary_distribution = fix_disconnected(counts,n_microstates,msm,pcca)\n",
    "    conf_seed = count_macro(n_sim,n_macro_disconnected,pcca_assignments,assignments,assignments_concat,sim_idx)\n",
    "    # Write files\n",
    "    write_gmxfile(sim_idx,n_sim,seed_ref,conf_seed)\n",
    "    # Run adaptive MD\n",
    "    gmx_mpirun(sim_name,sim_idx,n_sim,n_steps,ntomp,gpu_id)\n",
    "    write_features(colvar,sim_idx,n_sim,sim_name)\n",
    "    \n",
    "    ### CV convergence check & machine learning CV\n",
    "    if convergence_check == True:\n",
    "        sim_idx = sim_idx + 1\n",
    "        traj,data = read_features(colvar,sim_idx,n_sim)        # TODO: This is a repetitive action\n",
    "        data_supp = data_supplement(sim_idx,data,tica_lagtime)\n",
    "        # Machine learning CV\n",
    "        tica_i,tica_output_i,tica_output_concat_i,tica_output_supp_i = run_TICA(data,data_supp,tica_lagtime,num_cvs,None,koopman)\n",
    "        tica_cv_models.append(tica_i)\n",
    "        \n",
    "        correlations = cv_convergence(data,tica_lagtime,num_cvs,tica_cv_models,sim_idx)\n",
    "        np.savetxt('opes/correlations_round{sim_idx}.txt'.format(sim_idx=sim_idx-1),correlations)\n",
    "        \n",
    "        if (((correlations[-patience:][:,0].all() > convergence_criteria[0]) == True) and (correlations[-patience:][:,1].all() > convergence_criteria[1]) == True and sim_idx > patience) or sim_idx == n_rounds:\n",
    "            # Compute initial sigma for OPES simulation\n",
    "            sigma = calculate_cv_sigma(tica_output_concat_i,num_cvs)\n",
    "            # Write plumed files\n",
    "            tica_plumed(barrier,'traj_and_dat/features.dat',traj,tica_i,num_cvs,sigma)\n",
    "            break\n",
    "\n",
    "# Perform multiple-walkers OPES simulation\n",
    "opes_seed(tica_output_i,tica_output_concat_i,tica_output_supp_i,n_sim,n_jobs,sim_idx,seed_ref)\n",
    "opes_mpirun(opes_steps,sim_idx,n_steps,n_sim,ntomp,sim_name,gpu_id,num_cvs,barrier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
